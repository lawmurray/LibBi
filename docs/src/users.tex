\section{Introduction\label{Introduction}}

LibBi is used for Bayesian inference over state-space models, including
simulation, filtering and smoothing for state estimation, and optimisation and
sampling for parameter estimation.

LibBi supports state-space models of the form:
\begin{equation}\label{eqn:model}
\underbrace{p(\mathbf{y}_{1:T},\mathbf{x}_{0:T},\boldsymbol{\theta})}_{\text{joint}} =
\underbrace{\underbrace{p(\boldsymbol{\theta})}_{\text{parameter}}
\underbrace{p(\mathbf{x}_0|\boldsymbol{\theta})}_{\text{initial}}
\left(\prod_{t=1}^T
\underbrace{p(\mathbf{x}_t|\mathbf{x}_{t-1},\boldsymbol{\theta})}_{\text{transition}}\right)}_{\text{prior}}
\underbrace{\left(\prod_{t=1}^T \underbrace{p(\mathbf{y}_t|\mathbf{x}_t,\boldsymbol{\theta})}_{\text{observation}}\right)}_{\text{likelihood}}.
\end{equation}
where $t = 1,\ldots,T$ indexes time, $\mathbf{y}_{1:T}$ are observations,
$\mathbf{x}_{1:T}$ are state variables, and $\boldsymbol{\theta}$ are
parameters.

The state-space model in (\ref{eqn:model}) consists of four conditional
probability densities\index{probability\,density}:
\begin{itemize}
\item the \emph{parameter} model, specifying the prior density over parameters,
\item the \emph{initial} value model, specifying the prior density over the
  initial value of state variables, conditioned on the parameters,
\item the \emph{transition} model, specifying the transition density,
  conditioned on the parameters and previous state,
\item the \emph{observation} model, specifying the observation density,
  conditioned on the parameters and current state.
\end{itemize}
Each of these is explicitly specified using the LibBi modelling
language\seealso{See \secref{Models}{Models}}.

A brief example will help to set the scene. Consider the following
Lotka-Volterra-like\index{Lotka-Volterra} predator-prey\index{predator-prey}
model between zooplankton (predator, $Z$) and phytoplankton (prey, $P$):
\begin{eqnarray*}
\frac{dP}{dt} &=& \alpha_t P - cPZ\\
\frac{dZ}{dt} &=& ecPZ - m_lZ - m_q Z^2.
\end{eqnarray*}
Here, $t$ is time (in days), with prescribed constants $c = .25$, $e = .3$,
$m_l = .1$ and $m_q = .1$. The stochastic growth term, $\alpha_t$, is updated
in discrete time by drawing $\alpha_t \sim \mathcal{N}(\mu,\sigma)$
daily. Parameters to be estimated are $\mu$ and $\sigma$, and $P$ is observed,
with noise, at daily intervals.

The model above might be specified in the LibBi modelling language as follows:
\begin{bicode}
/**
 * Lotka-Volterra-like phytoplankton-zooplankton (PZ) model.
 */
model PZ \{
  const c = 0.25   // zooplankton clearance rate
  const e = 0.3    // zooplankton growth efficiency
  const m_l = 0.1  // zooplankton linear mortality
  const m_q = 0.1  // zooplankton quadratic mortality

  param mu, sigma  // mean and std. dev. of phytoplankton growth
  state P, Z       // phytoplankton, zooplankton
  noise alpha      // stochastic phytoplankton growth rate
  obs P_obs        // observations of phytoplankton
  
  sub parameter \{
    mu ~ uniform(0.0, 1.0)
    sigma ~ uniform(0.0, 0.5)
  \}
  
  sub initial \{
    P ~ log_normal(log(2.0), 0.2)
    Z ~ log_normal(log(2.0), 0.1)
  \}

  sub transition \{
    alpha ~ normal(mu, sigma)
    ode \{
      dP/dt = alpha*P - c*P*Z
      dZ/dt = e*c*P*Z - m_l*Z - m_q*Z*Z
    \}
  \}

  sub observation \{
    P_obs ~ log_normal(log(P), 0.2)
  \}
\}
\end{bicode}

This would be saved in a file named \bitt{PZ.bi}. Various tasks can now be
performed with the LibBi command-line interface, the simplest of which is
just sampling from the prior distribution of the model:
\begin{cmdcode}
libbi sample --target prior \textbackslash
    --model-file PZ.bi \textbackslash
    --nsamples 128 \textbackslash
    --end-time 365 \textbackslash
    --noutputs 365 \textbackslash
    --output-file results/prior.nc
\end{cmdcode}
This command will sample 128 trajectories of the model (\bitt{--nsamples
  128}), each of 365 days (\bitt{--end-time 365}), outputting the results
every day (\bitt{--noutputs 365}) to the NetCDF\index{NetCDF} file
\bitt{results/prior.nc}.

\begin{tip}
On the first occasion that a command is run, LibBi generates and compiles code
for you behind the scenes. This takes some time, depending on the complexity
of the model. The second time the command is run there is no such overhead,
and execution time is noticeably shorter. Changes to some command-line options
may also trigger a recompile.
\end{tip}

To play with this example further, download the \bitt{PZ} package from
\url{www.libbi.org}. Inspect and run the \texttt{run.sh} script to get
started.

The \secref{Command_line_interface}{command-line interface} provides numerous
other functionality, including filtering and smoothing the model with respect
to data, and optimising or sampling its parameters. The \clientref{help}
command is particularly useful, and can be used to access the contents of the
\secref{User_Reference}{User Reference} portion of this manual from the
command line.

\section{Getting started}\label{Getting_started}

There is a standard file and directory structure for a LibBi project. Using it
for your own projects ensures that they will be easy to share and distribute
as a LibBi package. To set up the standard structure, create an empty
directory somewhere, and from within that directory run:
\begin{cmdcode}
libbi package --create --name \emph{Name}
\end{cmdcode}
replacing \bitt{\emph{Name}} with the name of your project.

\begin{tip}
By convention, names always begin with an uppercase letter, and all new words
also begin with an uppercase letter, as in CamelCase. See the
\secref{Style_guide}{Style guide} for more such conventions.
\end{tip}

Each of the files that are created contains some placeholder content that is
intended to be modified. The \bitt{META.yml} file can be completed immediately
with the name of the package, the name of its author and a brief
description. This and other files are detailed with the \clientref{package}
command.

This early stage is also the ideal time to think about version control. LibBi
developers use Git\index{Git} for version control, and you may like to do the
same for your project. A new repository can be initialised in the same
directory with:
\begin{cmdcode}
git init
\end{cmdcode}
Then add all of the initial files to the repository and make the first commit:
\begin{cmdcode}
git add *
git commit -m 'Added initial files'
\end{cmdcode}
The state of the repository at each commit may be restored at any stage,
allowing old versions of files to be maintained without polluting the working
directory.

A complete introduction to Git is beyond the scope of this document. See
\href{http://www.git-scm.com}{www.git-scm.com} for more information. The
documentation for the \clientref{package} command also gives some advice on
what to include, and what not to include, in a version control repository.

The following command can be run at any time to validate that a project still
conforms to the standard structure:
\begin{cmdcode}
libbi package --validate
\end{cmdcode}

Finally, the following command can be used to build a package for
distribution:
\begin{cmdcode}
libbi package --build
\end{cmdcode}
This creates a \bitt{*.tar.gz} file in the current directory containing the
project files.

\section{Models\label{Models}}

Models are specified in the LibBi modelling language. A model specification is
put in a file with an extension of \bitt{*.bi}. Each such file contains only a
single model specification.

A specification always starts with an outer \kwref{model} statement that
declares and names the model. It then proceeds with declarations of constants,
dimensions and variables, followed by four \emph{top-level blocks} --
\blockref{parameter}, \blockref{initial}, \blockref{transition} and
\blockref{observation} -- that describe the factors of the state-space model.

A suitable template is:
\begin{bicode}
model \textsl{Name} \{
  // declare constants...
  // declare dimensions...
  // declare variables...

  sub parameter \{
    // specify the parameter model...
  \}

  sub initial \{
    // specify the initial condition model...
  \}

  sub transition \{
    // specify the transition model...
  \}

  sub observation \{
    // specify the observation model...
  \}
\}
\end{bicode}

Note that the contents of the \kwref{model} statement and each top-level block
are contained in curly braces (\bitt{\{\(\ldots\)\}}), in typical
C-style. Comments are also C-style, an inline comment being wrapped by
\bitt{/*} and \bitt{*/}, and the double-slash (\bitt{//}) denoting an
end-of-line comment. Lines may optionally end with a semicolon.

\subsection{Constants\label{Constants}}

\seealso{See also \secref{const}{\kwref{const}}}
Constants are named and immutable scalar values. They are declared using:
\begin{bicode}
const \emph{name} = \emph{constant_expression}
\end{bicode}
Often \bitt{\emph{constant\_expression}} is simply a literal value, but in
general it may be any constant scalar expression (see
\secref{Expressions}{Expressions}).

\subsection{Inlines\label{Inlines}}

\seealso{See also \secref{inline}{\kwref{inline}}}
Inlines are named scalar expressions. They are declared using:
\begin{bicode}
inline \emph{name} = \emph{expression}
\end{bicode}
Any use of the inline \bitt{\emph{name}} in subsequent expressions is
precisely equivalent to wrapping \bitt{\emph{expression}} in parentheses and
replacing \bitt{\emph{name}} with it. Inlines may be recursively nested.

\subsection{Dimensions\label{Dimensions}}

\seealso{See also \secref{dim}{\kwref{dim}}} Dimensions are used to construct
vector\index{vector}, matrix\index{matrix} and
higher-dimensional\index{higher\,dimensional} variables. Often, but not
necessarily, they have a spatial interpretation; for example a large
3-dimensional spatial model\index{spatial\,model} may declare dimensions
\bitt{x}, \bitt{y} and \bitt{z}. Dimensions are declared using:
\begin{bicode}
dim \emph{name}(\emph{size})
\end{bicode}

\begin{tip}
Because LibBi is primarily meant for state-space models, the time dimension is
special and not declared explicitly. Consider that, for continuous-time
models, time is not readily represented by a dimension of a finite size.
\end{tip}

\subsection{Variables\label{Variables}}

\seealso{See also \secref{input}{\kwref{input}, \kwref{param}, \kwref{state},
    \kwref{noise} and \kwref{obs}}} Variables are named and mutable
scalar\index{scalar}, vector\index{vector}, matrix\index{matrix} or
higher-dimensional\index{higher\,dimensional} objects.

A simple scalar variable is declared using:
\begin{bicode}
\emph{type} \emph{name}
\end{bicode}
where \bitt{\emph{type}} is one of:
\begin{description}
\item[\kwref{input}] for a variable with values that may or may
  not change over time, but that are prescribed according to input from a
  file,
\item[\kwref{param}] for a latent variable that does not change over time,
\item[\kwref{state}] for a latent variable that changes over time,
\item[\kwref{noise}] for a latent noise term that changes over time,
\item[\kwref{obs}] for an observed variable that changes over time.
\end{description}
For example, for the equation of the state-space model in the
\secref{Introduction}{Introduction}, the parameter $\theta$ may be declared
using:
\begin{bicode}
param theta
\end{bicode}
the state variable $x$ using:
\begin{bicode}
state x
\end{bicode}
and the observed variable $y$ using:
\begin{bicode}
obs y
\end{bicode}

A vector\index{vector}, matrix\index{matrix} or
higher-dimensional\index{higher\,dimensional} variable is declared by listing
the names of the \secref{Dimensions}{Dimensions} over which it extends,
separated by commas, in square brackets after the variable name. For example:
\begin{bicode}
dim m(50)
dim n(20)
...
state x[m,n]
\end{bicode}
declares a state variable \bitt{x}, which is a matrix\index{matrix} of size
$50 \times 20$.

The declaration of a variable can also include various arguments that control,
for example, whether or not it is included in output files. These are
specified in parentheses after the variable declaration, for example:
\begin{bicode}
state x[m,n](has_output = 0)
\end{bicode}

\subsection{Actions\label{Actions}}

Within each top-level block, a probability density is specified using
\emph{actions}. Simple actions take one of three forms:
\begin{bicode}
x ~ \textsl{expression}
x <- \textsl{expression}
dx/dt = \textsl{expression}
\end{bicode}
where \bitt{x} is some variable, referred to as the \emph{target}. Named
actions, available only for the first two forms, look like:
\begin{bicode}
x ~ \textsl{name}(\textsl{arguments}, ...)
x <- \textsl{name}(\textsl{arguments}, ...)
\end{bicode}

The first form, with the \bitt{\~{}} operator, indicates that the (random)
variable \bitt{x} is distributed according to the action given on the
right. Such actions are usually named, and correspond to parametric
probability distributions (e.g.  \actionref{gaussian}, \actionref{gamma} and
\actionref{uniform}). If the action is not named, the expression on the right
is assumed to be a probability density function.

The second form, with the \bitt{<-} operator, indicates that the
(deterministic) variable \bitt{x} should be assigned the result of the action
given on the right. Such actions might be simple scalar\index{scalar},
vector\index{vector} or matrix\index{matrix} expressions.

The third form, with the \bitt{=} operator, is for expressing ordinary
differential equations\index{ordinary\,differential\,equations}.

Individual elements of a vector, matrix or higher-dimensional variable may be
targeted using square brackets. Consider the following action giving the
ordinary differential equation\index{ordinary\,differential\,equations} for a
Lorenz '96\index{Lorenz\,'96} model:
\begin{bicode}
dx[i]/dt = x[i - 1]*(x[i + 1] - x[i - 2]) - x[i] + F
\end{bicode}
If \bitt{x} is a vector of size $N$, the action should be interpreted as ``for
each $i \in (0,\ldots,N-1)$, use the following ordinary differential
equation''. The name \bitt{i} is an \emph{index}, considered declared on the
left with local scope to the action, so that it may be used on the right. The
name \bitt{i} is arbitrary, but must not match the name of a constant, inline
or variable. It may match the name of a dimension, and indeed matching the
name of the dimension with which the index is associated can be sensible.

Elements of higher-dimensional variables may be targeted using multiple
indices. For example, consider computing a Euclidean distance matrix \bitt{D}:
\begin{bicode}
D[i,j] <- sqrt(pow(x[i] - x[j], 2) - pow(y[i] - y[j], 2))
\end{bicode}
Indexed actions such as this are useful for more complicated transformations
that either cannot be expressed in matrix\index{matrix} form, or that are
contrived when expressed as such.

Arguments to actions may be given with either \emph{positional}
\index{positional\,arguments} or \emph{named}\index{named\,arguments} forms,
or a mix of the two. Positional arguments are interpreted by the order
given. For example
\begin{bicode}
x ~ gaussian(0.0, 2.0)
\end{bicode}
means that \bitt{x} is distributed according to the \actionref{gaussian}
action with, by the definition of that action, mean $0.0$ and standard
deviation $2.0$. The \actionref{gaussian} action also happens to support named
arguments, so the following is equivalent:
\begin{bicode}
x ~ gaussian(mean = 0.0, std = 2.0)
\end{bicode}
The order of named arguments is unimportant, so the following is also
equivalent:
\begin{bicode}
x ~ gaussian(std = 2.0, mean = 0.0)
\end{bicode}

Positional and named arguments may be mixed, but all positional arguments must
appear before all named arguments. Thus, this is valid:
\begin{bicode}
x ~ gaussian(0.0, std = 2.0)
\end{bicode}
but these are not:
\begin{bicode}
x ~ gaussian(mean = 0.0, 2.0)
x ~ gaussian(std = 2.0, 0.0)
\end{bicode}
This avoids ambiguity in the syntax.

The documentation for an action lists its arguments, and whether it may be
used in named or positional form, or both.

\subsection{Blocks}

Some actions are more complicated and can, or must, be wrapped in
\emph{blocks}. Good examples are systems of ordinary differential
equations\index{ordinary\,differential\,equations}. Two or more equations must
be grouped to form the complete system. The grouping is achieved with the
\blockref{ode} block.

Consider the following Lotka-Volterra-type\index{Lotka-Volterra} transition
model:
\begin{bicode}
sub transition \{
  sub ode \{
    dP/dt = alpha*P - c*P*Z
    dZ/dt = e*c*P*Z - m_l*Z
  \}
\}
\end{bicode}
Here, the \blockref{ode} block combines the two actions into the one system of
ordinary differential equations.

Like actions, blocks can take arguments. The \blockref{ode} block has
parameters to select and configure the algorithm used to numerically integrate
the differential equations forward through time. If the defaults are
inadequate, these might be set as follows:
\begin{bicode}
sub transition \{
  sub ode(atoler = 1.0e-3, rtoler = 1.0e-3, alg = 'dopri5') \{
    dP/dt = alpha*P - c*P*Z
    dZ/dt = e*c*P*Z - m_l*Z
  \}
\}
\end{bicode}
As for actions, both positional and named forms of arguments are supported.

\subsection{Expressions\label{Expressions}}

LibBi supports expressions over scalars, vectors, matrices and
higher-dimensional variables:
\begin{itemize}
\item A \textit{scalar}\index{scalar} is a literal, constant or variable that
  is not declared over any dimensions.
\item A \textit{vector}\index{vector} is a variable declared over
  exactly one dimension.
\item A \textit{matrix}\index{matrix} is a variable declared over exactly two
  dimensions.
\item Variables declared over three or more dimensions are not given a special
  name.
\end{itemize}
Note that, by these definitions, a variable declared over a single dimension
of size one is considered a vector, not a scalar. Any variable declared over
more than one dimension, where all of those dimensions have size one, is
likewise not considered a scalar. The reason for this is that the particular
boundary conditions of those dimensions may convey different behaviour to that
of a scalar.

Special classes of expression are:
\begin{itemize}
\item A \textit{constant expression}\index{constant\,expression} is one that
  can be evaluated at compile time. It must be scalar, and may refer to
  literals, constants and inlines that expand to other constant expressions
  only.
\item A \textit{static expression}\index{static\,expression} is one that does
  not depend on time. It may refer to literals, constants, variables of type
  \kwref{param}, and inlines that expand to other static expressions only.
\item A \textit{common expression}\index{common\,expression} is one that does
  not depend on the state of a particular trajectory. It may refer to
  literals, constants, variables of type \kwref{param} or \kwref{input}, and
  inlines that expand to other common expressions only.
\end{itemize}
Note from these definitions that a constant expression is a static expression,
and a static expression is a common expression.

%\subsection{Operators\label{Operators}}

The following operators are supported in expressions:

\noindent
\begin{tabular}{lr}
\hline
Scalar/vector/matrix arithmetic operators: & \bitt{+ - * / \% **} \\
Element-wise vector/matrix operators:  & \bitt{.+ .- .* ./ .\% .**} \\
%Bitshift operators: & \bitt{<< >>} \\
Comparison operators: & \bitt{== != < <= > >= } \\
Logical operators: & \bitt{\&\& ||} \\
Ternary operators: & \bitt{?:} \\
\hline
\end{tabular}\index{scalar}\index{vector}\index{matrix}\index{arithmetic}\index{operators}

%\subsection{Functions\label{Functions}}

The following functions are supported in expressions:

\noindent
\begin{tabular}{p{\textwidth}}
\hline \bitt{abs acos acosh asin asinh atan atan2 atanh ceil cos cosh erf erfc exp
  floor gamma lgamma log max min mod pow round sin sinh sqrt tan tanh}
\\ \hline
\end{tabular}\index{functions}

\section{Command-line interface\label{Command_line_interface}}\index{command\,line}

Methods are applied to models via the command-line interface of LibBi. This is
invoked using:
\begin{cmdcode}
libbi \textit{command} \textit{options} ...
\end{cmdcode}
where \bitt{\textit{command}} is any one of the following:
\begin{description}
\item[\clientref{filter}] for filtering problems using the model and
  observations,
%\item[\clientref{smooth}] for smoothing problems using the model and
%  observations,
\item[\clientref{optimise}] for parameter optimisation problems using the
  model and observations,
\item[\clientref{sample}] for parameter and state sampling problems using the
  model and observations,
\item[\clientref{package}] for creating projects and building packages for
  distribution,
\item[\clientref{help}] for accessing online help,
\item[\clientref{draw}] to visualise a model (useful for
  debugging and development),
\item[\clientref{rewrite}] to inspect the internal representation of a model
  (useful for debugging and development),
\end{description}
and available \bitt{\textit{options}} depend on the command.

Options may be specified in a configuration file\index{config\,file} or on the
command line itself. To use a configuration file, give the name of the file on
the command line, preceded by \bitt{@}, e.g.
\begin{cmdcode}
libbi \textit{command} @command.conf
\end{cmdcode}

More than one config file may be specified, each preceded by \bitt{@}. An
option given on the command line will override an option of the same name
given in the configuration file.

A config file simply contains a list of command-line options just as they
would be given on the command line itself. For readability, the command-line
options may be spread over any number of lines, and end-of-line comments,
preceded by \bitt{\#}, may appear. The contents of one config file may be
nested in another by using the \bitt{@file.conf} syntax within a file. This
can be useful to avoid redundancy. For example, the \clientref{sample} command
inherits all the options of \clientref{filter}. In this case it may be useful
to write a \bitt{filter.conf} file that is nested within a \bitt{sample.conf}
file, so that options need not be repeated.

\section{Output files}

LibBi uses NetCDF\index{NetCDF} files for input and output\index{I/O}. A
NetCDF file consists of \emph{dimensions}, along with \emph{variables}
extending across them. This is very similar to LibBi models. Where use of
these overloaded names may cause confusion, the specific names \emph{NetCDF
  variable/dimension} and \emph{model variable/dimension} are used throughout
this section and the next. A NetCDF file may also contain scalar
\emph{attributes} that provide meta information.

The \emph{schema} of a NetCDF file is its structure with respect to the
dimensions, variables and attributes that it contains. Among the dimensions
will be those that correspond to model dimensions. Likewise for
variables. Some additional dimensions and variables will provide critical
method-specific information or diagnostics.

\begin{tip}
To quickly inspect the structure and contents of a NetCDF file, use the
\bitt{ncdump} utility, included as standard in NetCDF distributions:
\begin{cmdcode}
ncdump file.nc | less
\end{cmdcode}
\end{tip}

LibBi output files have several schemas. The schema of an output file depends
on the command that produces it, and the options given to that command. The
schemas are related by strong conventions and inherited structure.

This section outlines each schema in turn. The syntax \bitt{x[m,n]} is used to
refer to a NetCDF variable named \bitt{x} that extends across the NetCDF
dimensions named \bitt{m} and \bitt{n}.

\begin{tip}
If using OctBi or RBi for collating and visualising the output of LibBi, it
may be unnecessary to understand these schema, as they are encapsulated by
the higher-level functions of these packages.
\end{tip}

\subsection{Simulation schema\label{Simulation schema}}

This schema is used by the \clientref{sample} command when the \bitt{target}
option is set to \bitt{prior}, \bitt{joint} or \bitt{prediction}. It consists
of dimensions:
\begin{itemize}
\item \bitt{nr} indexing time,
\item \bitt{np} indexing trajectories, and
\item for each dimension \bitt{\emph{n}} in the model, a dimension
  \bitt{\emph{n}}.
\end{itemize}
And variables:
\begin{itemize}
\item \bitt{time[nr]} giving the output times,
\item for each \kwref{param} variable \bitt{\emph{theta}} in the model,
  defined over dimensions \bitt{\emph{m},...,\emph{n}}, a variable
  \bitt{\emph{theta}[\emph{m},...,\emph{n}]}, and
\item for each \kwref{state} and \kwref{noise} variable \bitt{\emph{x}} in the
  model, defined over dimensions \bitt{\emph{m},...,\emph{n}}, a variable
  \bitt{\emph{x}[nr,\emph{m},...,\emph{n},np]}.
\end{itemize}

\subsection{Particle filter schema}

This schema is used by the \clientref{filter} command when a particle filter,
besides the adaptive particle filter, is chosen. It extends the simulation
schema, with the following changes:
\begin{itemize}
\item the \bitt{np} dimension is now interpreted as indexing \emph{particles}
  not \emph{trajectories}.
\end{itemize}

The following variables are added:
\begin{itemize}
\item \bitt{logweight[nr,np]} giving the log-weights vector at each time, and
\item \bitt{ancestor[nr,np]} giving the ancestry vector at each time.
\end{itemize}

\begin{tip}
To draw a whole trajectory out of a file of the particle filter schema, begin
at the last time, select a particle, and use the \bitt{ancestor} vector at
each time to trace that particle's ancestry back through time. One cannot
simply take a row from the matrix of a state variable to obtain a complete
trajectory, as with the simulation schema.
\end{tip}

\subsection{Simulation ``flexi'' schema}

This schema is currently not used directly, but the particle filter ``flexi''
schema below extends it. The complication here is that the number of
particles at each time can vary. The schema consists of the following
dimensions:
\begin{itemize}
\item \bitt{nr} indexing time,
\item \bitt{nrp} indexing both time and trajectories (an unlimited dimension),
  and
\item for each dimension \bitt{\emph{n}} in the model, a dimension
  \bitt{\emph{n}}.
\end{itemize}
And variables:
\begin{itemize}
\item \bitt{time[nr]} giving the output times,
\item \bitt{start[nr]} giving, for each time, the starting index along the
  \bitt{nrp} dimension for particles associated with that time,
\item \bitt{len[nr]} giving, for each time, the number of particles at that
  time,
\item for each \kwref{param} variable \bitt{\emph{theta}} in the model,
  defined over dimensions \bitt{\emph{m},...,\emph{n}}, a variable
  \bitt{\emph{theta}[\emph{m},...,\emph{n}]}, and
\item for each \kwref{state} and \kwref{noise} variable \bitt{\emph{x}} in the
  model, defined over dimensions \bitt{\emph{m},...,\emph{n}}, a variable
  \bitt{\emph{x}[\emph{m},...,\emph{n},nrp]}.
\end{itemize}

\begin{tip}
With the ``flexi'' schema, to read the contents of a variable at some time
index \bitt{\emph{t}}, read \bitt{len[\emph{t}]} entries along the \bitt{nrp}
dimension, beginning at index \bitt{start[\emph{t}]}.
\end{tip}

\subsection{Particle filter ``flexi'' schema}

This schema is used by the \clientref{filter} command when an adaptive
particle filter is chosen. It extends the simulation ``flexi'' schema. The
following variables are added:
\begin{itemize}
\item \bitt{logweight[nrp]} giving the log-weights vector at each time, and
\item \bitt{ancestor[nrp]} giving the ancestry vector at each time.
\end{itemize}

\subsection{Kalman filter schema}

This schema is used by the \clientref{filter} command when a Kalman filter is
chosen. It extends the simulation schema, with the following changes:
\begin{itemize}
\item the \bitt{np} dimension is always of size 1, and
\item variables that correspond to \kwref{state} and \kwref{noise} variables
  in the model are now interpreted as containing the mean of the filter
  density at each time.
\end{itemize}

The following dimensions are added:
\begin{itemize}
\item \bitt{nxcol} indexing columns of matrices, and
\item \bitt{nxrow} indexing rows of matrices.
\end{itemize}

The following variables are added:
\begin{itemize}
\item \bitt{U\_[nr,nxcol,nxrow]} containing the upper-triangular Cholesky
  factor of the covariance matrix of the filter density at each time.
\item For each \kwref{noise} and \kwref{state} variable \bitt{\emph{x}}, a
  variable \bitt{index.\emph{x}} giving the index of the first row (and
  column) in \bitt{S\_} pertaining to this variable, the first such index being
  zero.
\end{itemize}

\subsection{Optimisation schema}

This schema is used by the \clientref{optimise} command. It consists of
dimensions:
\begin{itemize}
\item \texttt{np} indexing iterations of the optimiser (an unlimited
  dimension).
\end{itemize}
And variables:
\begin{itemize}
\item for each \kwref{param} variable \bitt{\emph{theta}} in the model,
  defined over dimensions \bitt{\emph{m},...,\emph{n}}, a variable
  \bitt{\emph{theta}[\emph{m},...,\emph{n},np]},
\item \bitt{optimiser.value[np]} giving the value of the function being
  optimised at each iteration, and
\item \bitt{optimiser.size[np]} giving the value of the convergence criterion
  at each iteration.
\end{itemize}

\subsection{PMCMC schema}

This schema is used by the \clientref{sample} command when a PMCMC method is
chosen. It extends the simulation schema, with the following changes:
\begin{itemize}
\item the \bitt{np} dimension indexes samples, not trajectories, and
\item for each \kwref{param} variable \bitt{\emph{theta}} in the model,
  defined over dimensions \bitt{\emph{m},...,\emph{n}}, there is a variable
  \bitt{\emph{theta}[\emph{m},...,\emph{n},np]} instead of
  \bitt{\emph{theta}[\emph{m},...,\emph{n}]} (i.e. \kwref{param} variables are
  defined over the \bitt{np} dimension also).
\end{itemize}
The following variables are added:
\begin{itemize}
\item \bitt{loglikelihood[np]} giving the log-likelihood estimate
  $\hat{p}(\mathbf{y}_{1:T}|\boldsymbol{\theta}^p)$ for each sample
  $\boldsymbol{\theta}^p$, and
\item \bitt{logprior[np]} giving the log-prior density
  $p(\boldsymbol{\theta}^p)$ of each sample $\boldsymbol{\theta}^p$.
\end{itemize}

\subsection{SMC$^2$ schema}

This schema is used by the \clientref{sample} command when an SMC$^2$ method
is chosen. It extends the PMCMC schema, with the following additional
variables:
\begin{itemize}
\item \bitt{logweight[np]} giving the log-weights vector of parameter samples.
\item \bitt{logevidence[nr]} giving the incremental log-evidence of the model
  at each time.
\end{itemize}

\section{Input files\label{Input_files}}\index{data}\index{I/O}\index{input}

Input files take three forms:
\begin{itemize}
\item initialisation files, containing the initial values of \kwref{state}
  variables,
\item input files, containing the values of \kwref{input} variables, possibly
  changing across time, and
\item observation files, containing the observed values of \kwref{obs}
  variables.
\end{itemize}
All of these use the same schema. That schema is quite flexible, allowing for
the representation of both dense and sparse input. Sparsity may be in time or
space. Sparsity in time is, for example, having a discrete-time model with a
scalar \kwref{obs} variable that is not necessary observed at all
times. Sparsity in space is, for example, having a discrete-time model with a
vector \kwref{obs} variable, for which not all of the elements are necessarily
observed at all times.

Each model variable is associated with a NetCDF variable of the same
name. Additional NetCDF variables may exist. Model variables which cannot be
matched to a NetCDF variable of the same name do not receive input from the
file. For \kwref{input} variables, this means that they will remain
uninitialised. For spatially dense input, each such NetCDF variable may be
defined along the following dimensions, in the order given:
\begin{enumerate}
\item Optionally, a dimension named \bitt{ns}, used to index multiple
  experiments set up in the same file. If not given for a variable, that
  variable is assumed to be the same for all experiments.
\item Optionally, a time dimension (see below).
\item Optionally, a number of dimensions with names that match the model
  dimensions along which the model variable is defined, ordered accordingly,
\item Optionally, a dimension named \bitt{np}, used to index multiple
  trajectories (instantiations, samples, particles) of a variable. If not
  given for a variable, the value of that variable is assumed to be the same
  for all trajectories. Variables of type \kwref{param}, \kwref{input} and
  \kwref{obs} may not use an \bitt{np} dimension, as by their nature they are
  meant to be in common across all trajectories.
\end{enumerate}

For spatially sparse input, see \secref{Coordinate_variables}{Coordinate
  variables} below.

\begin{tip}
The \secref{Simulation schema}{Simulation schema} is in fact a special case of
the input schema, so that the \clientref{sample} command can often be used
quite sensibly as input to another run. For example, a simulated data set can
be generated with:
\begin{cmdcode}
libbi sample \textbackslash
    --target joint \textbackslash
    --nsamples 1 \textbackslash
    --output-file data/obs.nc \textbackslash
    ...
\end{cmdcode}
The \bitt{data/obs.nc} file can then be used as an observation file, sampling
from the posterior distribution conditioned on the simulated trajectory as
data:
\begin{cmdcode}
libbi sample \textbackslash
    --target posterior \textbackslash
    --obs-file data/obs.nc \textbackslash
    --output-file results/posterior.nc \textbackslash
    ...
\end{cmdcode}
Then, indeed, the output of that might be fed into a new sample forward in
time:
\begin{cmdcode}
libbi sample \textbackslash
    --target prediction \textbackslash
    --init-file results/posterior.nc \textbackslash
    --output-file results/prediction.nc \textbackslash
    ....
\end{cmdcode}
\end{tip}

\subsection{Time variables\label{Time_variables}}

\emph{Time variables} are used to index time in a file. Each NetCDF variable
with a name beginning with ``time'' is assumed to be a time variable. Each
such variable may be defined along the following dimensions, in the order
given:
\begin{enumerate}
\item Optionally, the \bitt{ns} dimension.
\item An arbitrarily named dimension.
\end{enumerate}
The latter dimension becomes a \emph{time
  dimension}\index{time\,dimension}. The time variable gives the time
associated with each index of that time dimension, a sequence which must be
monotonically non-decreasing. NetCDF variables that correspond to model
variables, and that are defined along the same time dimension, become
associated with the time variable. The time dimension thus enumerates both the
times at which these variables change, and the values that they are to assume
at those times. A variable may only be associated with one time dimension, and
\texttt{param} variables may not be associated with one at all. If a variable
is not defined across a time dimension, it is assumed to have the same value
at all times.

Time variables and time dimensions are interpreted slightly differently for
each of the input file types:
\begin{enumerate}
\item For an initialisation file, the starting time (given by the
  \bitt{--start-time} command-line option, see \clientref{filter}) is
  looked-up in each time variable, and the corresponding record in each
  associated variable is used for its initialisation.
\item For input files, a time variable gives the times at which each
  associated variable changes in value. Each variable maintains its new
  value until the time of the next change.
\item For observation files, a time variable gives the times at which each
  associated variable is observed. The value of each variable is interpreted
  as being its value at that precise instant in time.
\end{enumerate}

\begin{example}
\noindent \textbf{Representing a scalar input}

Assume that we have an \kwref{obs} variable named \bitt{y}, and we wish to
construct an observation file containing our data set, which consists of
observations of \bitt{y} at various times. A valid NetCDF schema would be:
\begin{itemize}
\item a dimension named \bitt{nr}, to be our time dimension,
\item a variable named \bitt{time\_y}, defined along the dimension \bitt{nr},
  to be our time variable, and
\item a variable named \bitt{y}, defined along the dimension \bitt{nr}, to
  contain the observations.
\end{itemize}
We would then fill the variable \bitt{time\_y} with the observation times of
our data set, and \bitt{y} with the actual observations. It may look something
like this:

\begin{cmdcode}
time_y[nr]    y[nr]
       0.0     6.75
       1.0     4.56
       2.0     9.45
       5.5     4.23
       6.0     7.12
       9.5     5.23
\end{cmdcode}

\end{example}

\begin{example}
\noindent \textbf{Representing a vector input, densely}

Assume that we have an \kwref{obs} variable named \bitt{y}, which is a vector
defined across a dimension of size three called \bitt{n}. We wish to construct
an observation file containing a our data set, which consists of observations
of \bitt{y} at various times, where at each time all three elements of
\bitt{y} are observed. A valid NetCDF schema would be:
\begin{itemize}
\item a dimension named \bitt{nr}, to be our time dimension,
\item a variable named \bitt{time\_y}, defined along the dimension \bitt{nr},
  to be our time variable,
\item a dimension named \bitt{n}, and
\item a variable named \bitt{y}, defined along the dimensions \bitt{nr} and
  \bitt{n}, to contain the observations.
\end{itemize}
We would then fill the variable \bitt{time\_y} with the observation times of
our data set, and \bitt{y} with the actual observations. It may look something
like this:

\begin{cmdcode}
time_y[nr]            y[nr,n]
       0.0     6.75 3.34 3.45
       1.0     4.56 4.54 1.34
       2.0     9.45 3.43 1.65
       5.5     4.23 8.65 4.64
       6.0     7.12 4.56 3.53
       9.5     5.23 3.45 3.24
\end{cmdcode}

\end{example}

\subsection{Coordinate variables\label{Coordinate_variables}}

\emph{Coordinate variables} are used for spatially sparse input. Each variable
with a name beginning with ``coord'' is assumed to be a coordinate
variable. Each such variable may be defined along the following dimensions, in
the order given:
\begin{enumerate}
\item Optionally, the \bitt{ns} dimension.
\item A coordinate dimension (see below).
\item Optionally, some arbitrary dimension.
\end{enumerate}
The second dimension, the \emph{coordinate
  dimension}\index{coordinate\,dimension}, may be a time dimension as
well. NetCDF variables that correspond to model variables, and that are
defined along the same coordinate dimension, become associated with the
coordinate variable. The coordinate variable is used to indicate which
elements of these variables are active. The last dimension, if any, should
have a length equal to the number of dimensions across which these variables
are defined. So, for example, if these variables are matrices, the last
dimension should have a length of two. If the variables are vectors, so that
they have only one dimension, the coordinate variable need not have this last
dimension.

%If a multidimensional variable is associated with a coordinate variable with
%fewer components than the number of dimensions along which it is defined,
%these are assumed to index the outermost dimensions. That is, a variable may
%be sparse in some dimensions and dense in others, but the sparse dimensions
%must be the outermost.

If a variable specified across one or more dimensions in the model cannot be
associated with a coordinate variable, then it is assumed to be represented
densely.

\begin{example}
\noindent \textbf{Representing a vector input, sparsely}

Assume that we have an \kwref{obs} variable named \bitt{y}, which is a vector
defined across a dimension of size three called \bitt{n}. We wish to construct
an observation file containing our data set, which consists of observations of
\bitt{y} at various times, where at each time only a subset of the elements of
\bitt{y} are observed. A valid NetCDF schema would be:
\begin{itemize}
\item a dimension named \bitt{nr}, to be both our time and coordinate
  dimension,
\item a variable named \bitt{time\_y}, defined along the dimension \bitt{nr},
  to be our time variable,
\item a variable named \bitt{coord\_y}, defined along the dimension \bitt{nr},
  to be our coordinate variable,
\item a variable named \bitt{y}, defined along the dimension \bitt{nr}, to
  contain the observations.
\end{itemize}
We would then fill the variable \bitt{time\_y} with the observation times of
our data set, \bitt{coord\_y} with the coordinate of each observation, and
\bitt{y} with the observations themselves. It may look something like this:

\begin{cmdcode}
time_y[nr]   coord_y[nr]      y[nr]
       0.0             0       6.75
       0.0             1       3.34
       1.0             0       4.56
       1.0             1       4.54
       1.0             2       1.34
       2.0             1       3.43
       5.5             3       4.64
       6.0             0       4.23
       6.0             2       3.53
       9.5             1       3.45
\end{cmdcode}

Note that each unique value in \bitt{time\_y} is repeated for as many
coordinates as are active at that time. Also note that, if \bitt{y} had $m >
1$ dimensions, the \bitt{coord\_y} variable would be defined along some
additional, arbitrarily named dimension of size $m$ in the NetCDF file, so
that the values of \bitt{coord\_y} in the above table would be vectors.
\end{example}

\subsection{Sampling models with input\label{Sampling_models_with_input}}

The precise way in which input files and the model specification interact is
best demonstrated in the steps taken to sample a model's prior
distribution. Computing densities is similar. The \emph{initialisation file}
referred to in the proceeding steps is that given by the \bitt{--init-file}
command-line option, and the \emph{input file} that given by
\bitt{--input-file}.
\begin{enumerate}
\item Any \kwref{input} variables in the input file that are not associated with
  a time variable are initialised by reading from the file.
\item The \blockref{parameter} top-level block is sampled.
\item Any \kwref{param} variables in the initialisation file are overwritten
  by reading from the file.
\item The \blockref{initial} top-level block is sampled.
\item Any \kwref{state} variables in the initialisation file are overwritten
  by reading from the file.
\item The \blockref{transition} top-level block is sampled forward through
  time. Sampling stops at each time that an \kwref{input} variable is to
  change, according to the input file, at which point the \kwref{input}
  variable is updated and sampling of the \blockref{transition} block
  continues.
\end{enumerate}

Note two important points in this procedure:
\begin{itemize}
\item An \kwref{input} variable in the input file that is not associated with a
  time variable is initialised before anything else, whereas an \kwref{input}
  variable that is associated with a time variable is not initialised until
  simulation begins, even if the first entry of that variable indicates an
  update at time zero. This has implications as to which \kwref{input}
  variables are, or are not, initialised at the time that the
  \blockref{parameter} block is sampled.
\item While the \blockref{parameter} and \blockref{initial} blocks are always
  sampled, the samples may be later overwritten from the initialisation
  file. Thus, the initialisation file need not contain a complete set of
  variables, although behaviour is more intuitive if it does. This behaviour
  also ensures \secref{Pseudorandom_reproducibility}{pseudorandom
    reproducibility} regardless of the presence, or content, of the
  initialisation file.
\end{itemize}

\section{Getting it all working\label{Getting it all working}}

This section contains some general advice on the statistical methods employed
by LibBi and the tuning that might be required to make the most of them. It
concentrates on the \emph{particle marginal
  Metropolis-Hastings}\index{Metropolis-Hastings} (PMMH) sampler, used by
default by the \clientref{sample} command when sampling from the posterior
distribution.

PMMH is of the family of \emph{particle Markov chain Monte Carlo} (PMCMC)
methods~\citep{Andrieu2010}, which in turn belong to the family of Markov
chain Monte Carlo (MCMC)\index{Markov\,chain\,Monte\,Carlo}. A complete
introduction to PMMH is beyond the scope of this manual. \citet{Murray2013b}
provides an introduction of the method and its implementation in LibBi.

When using MCMC methods it is common to perform some short pilot runs to tune
the parameters of the method in order to improve its efficiency, before
performing a final run. In PMMH, the parameters to be tuned are the proposal
distribution, and the number of particles in the particle filter, or
sequential Monte Carlo~\citep{Doucet2001}, component of the method.

When running PMMH in LibBi, diagnostics are output that can be used to guide
tuning. Here is an example:

{\scriptsize\begin{cmdcode}
22: -116.129  -16.984  7.25272  beats -121.853  -17.6397  7.49326  accept=0.217391
23: -116.129  -16.984  5.63203  beats -119.772  -18.0891  6.07209  accept=0.208333
24: -116.129  -16.984  6.89478  beats -121.268  -19.3354  8.25723  accept=0.2
25: -116.129  -16.984  0.643236 beats -136.661  -21.5339  6.44331  accept=0.192308
26: -116.129  -16.984  3.58096  beats -128.692  -20.3304  6.3502   accept=0.185185
\end{cmdcode}}

The numerical columns provide, in order:
\begin{enumerate}
\item the iteration number,
\item the log-likelihood of the current state of the chain,
\item the prior log-density of the current state of the chain,
\item the proposal log-density of the current state of the chain, conditioned
  on the other state,
\item the log-likelihood of the other state of the chain (the previous state
  if the most recent proposal was accepted, the last proposed state if the
  most recent proposal was rejected),
\item the prior log-density of the other state of the chain,
\item the proposal log-density of the other state of the chain, conditioned on
  the current state, and
\item the acceptance rate of the chain so far.
\end{enumerate}
The last of these is the most important for tuning.

For a standard Metropolis-Hastings, a reasonable guide is to aim at an
acceptance rate of 0.5 for a single parameter, down to 0.23 for five or more
parameters~\citep{Gelman1994}. This includes the case where a Kalman filter is
being used rather than a particle filter (by using the \bitt{--filter kalman}
option to \clientref{sample}). In such cases the only tuning to perform is
that of the proposal distribution. The proposal distribution is given in the
\blockref{proposal_parameter} block of the model specification. If this block
is not specified, the \blockref{parameter} block is used instead, and this may
make for a poor proposal distribution, especially when there are many
observations. Increasing the width of the proposal distribution will decrease
the acceptance rate. Decreasing the width of the proposal distribution will
increase the acceptance rate.

\begin{tip}
Higher acceptance rates are not necessarily better. They may simply be a
result of the chain exploring the posterior distribution very slowly.
\end{tip}

PMMH has the added complication of using a particle filter to estimate the
likelihood, rather than a Kalman filter to compute it exactly (although note
that the Kalman filter works only for linear and Gaussian models). It is
necessary to set the number of particles in the particle filter. More
particles decreases the variance in the likelihood estimator and so increases
the acceptance rate, but also increases the computational cost. Because the
likelihood is estimated and not computed exactly, the optimal acceptance rate
will be lower than for standard Metropolis-Hastings. Anecdotally, 0.1--0.15
seems reasonable.

The tradeoff between proposal size and number of particles is still under
study, e.g. \citet{Doucet2013}. The following procedure is suggested.
\begin{enumerate}
\item Start with an empty \blockref{proposal_parameter} block. Set the
  simulation time (\bitt{--end-time}) to the time of the first observation,
  and the number of particles (\bitt{--nparticles}) to a modest amount. When
  running PMMH, it is then the case that the same state of the chain, its
  starting state in fact, will be proposed repeatedly, and the acceptance rate
  will depend entirely on the variance of the likelihood estimator. One hopes
  to see a high acceptance rate here, say 0.5 or more. Increase the number of
  particles until this is achieved. Note that the random number seed can be
  fixed (\bitt{--seed}) if you wish.
\item Steadily extend the simulation time to include a few more observations
  on each attempt, and increase the number of particles as needed to maintain
  the high acceptance rate. The number of particles will typically need to
  scale linearly with the simulation time. Consult the \secref{Performance
    guide}{Performance guide} to improve execution times and further increase
  the number of particles if necessary.
\item If a suitable configuration is achieved for the full data set, or a
  workable subset, the proposal distribution can be considered. You may find
  it useful to add one parameter at a time to the proposal distribution,
  working towards an overall acceptance rate of 0.1--0.15.
\item If this fails to find a working combination with a healthy acceptance
  rate, consider the initialisation of the chain. By default, LibBi simply
  draws a sample from the parameter model to initialise the chain. If this is
  in an area where the variance in the likelihood estimator is high, the chain
  may mix untenably slowly for any sensible number of particles. It has been
  observed empirically that the variance in the likelihood estimator is
  heteroskedastic, and tends to increase with distance from the maximum
  likelihood~\citep{Murray2013a}. So initialising the chain closer to the
  maximum likelihood may allow it to mix well with a reasonable number of
  particles. Prior knowledge, optimisation of parameters (perhaps with the
  \clientref{optimise} command), or exploration of the data set may inform the
  initialisation. The initialisation can be given in the initialisation file
  (\bitt{--init-file}).
\end{enumerate}

\begin{tip}
It can also be the case that, in steadily extending the simulation time
(\bitt{--end-time}), the acceptance rate suddenly drops at a particular
time. This indicates that the particle filter degenerates at this
point. Improving the initialisation of the chain is the best strategy in this
case, although increasing the number of particles may help in mild cases.
\end{tip}

Be aware that LibBi uses methods that are still being actively developed, and
applied to larger and more complex models. It may be the case that your model
or data set exceeds the current capabilities of the software. In such cases
the only option is to consider a smaller or simpler model, or a subsample of
the available data.

\section{Performance guide\label{Performance guide}\index{performance}}

One of the aims of LibBi is to alleviate the user from performance
considerations as much as possible. Nevertheless, there is some scope to
influence performance, particularly by ensuring that appropriate hardware
resources are used, and by limiting I/O where possible.

\subsection{Precomputing}

LibBi will:
\begin{itemize}
\item precompute constant subexpressions, and
\item precompute static subexpressions in the transition and observation
  models.
\end{itemize}
Reducing redundant or repetitious expressions is thus unnecessary where these
are constant\index{constant\,expression} or
static\index{static\,expression}. For example, taking the square-root of a
variance parameter need not be of concern:
\begin{bicode}
param sigma2
\(\ldots\)
sub transition \{
  epsilon ~ gaussian(mu, sqrt(sigma2))
  \(\ldots\)
\}
\end{bicode}
Here, \bitt{sqrt(sigma2)} is a static expression\index{static\,expression}
that will be extracted and precomputed outside of the transition block.

\begin{tip}
Use the \clientref{rewrite} command to inspect precisely which expressions
have been extracted for precomputation.
\end{tip}

\subsection{I/O}

The following I/O options are worth considering to reduce the size of output
files and so the time spent writing to them:
\begin{itemize}
\item When declaring a variable, use a \bitt{has\_output = 0} argument to omit
  it from output files if it will not be of interest.
\item Enable \index{single\,precision} output files by using the
  \bitt{--enable-single} command-line option. This will reduce write size by
  up to a half. Note, however, that all computations are then performed in
  single precision too, which may have significant numerical implications.
\end{itemize}

\subsection{Configuration}

The following configuration options are worth considering:
\begin{itemize}
\item Internally, LibBi uses extensive assertion checking to catch programming
  and code generation errors\index{assertion\,checking}. These assertion
  checks are enabled by default, improving robustness at the expense of
  performance. It is recommended that they remain enabled during model
  development and small-scale testing, but that they are disabled for final
  production runs. They can be disabled by adding the \bitt{--disable-assert}
  command-line option.

\item Experiment with the \bitt{--enable-cuda} command-line option to make use
  of a CUDA-enabled GPU\index{GPU}\index{CUDA}. This should usually improve
  performance, as long as a sufficient number of model trajectories are to be
  simulated (typically upwards of 1024). If CUDA is being used, also try the
  \bitt{--enable-gpu-cache} command-line option if running PMCMC or SMC$^2$.
  This caches particle histories in GPU rather than main memory. As GPU
  memory is usually much more limited than main memory, and this may result in
  its exhaustion, the option is disabled by default.

\item Experiment with the \bitt{--enable-sse} command-line
  option to make use of CPU SSE instructions\index{SSE}\index{SIMD}. These can
  provide up to a two-fold (double precision) or four-fold (single precision)
  speed-up.

\item \index{multithreading}\index{OpenMP} Experiment with the
  \bitt{--threads} command-line option to set the number of CPU
  threads. Typically there are depreciating gains as the number of threads is
  increased, and beyond the number of physical CPU cores performance will
  degrade significantly. For CPUs with hyperthreading enabled, it is
  recommended that the number of threads is set to no more than the number of
  physical CPU cores. This may be half the default number of threads.

\item Experiment with using single precision\index{single\,precision} floating
  point operations by using the \bitt{--enable-single} command-line
  option. This can offer significant performance improvements (especially when
  used in conjunction with the \bitt{--enable-cuda} and \bitt{--enable-sse}
  options), but care should be taken to ensure that numerical error remains
  tolerable. The use of single precision will also reduce memory consumption
  by up to a half.

\item Use optimised versions of libraries, especially the BLAS\index{BLAS} and
  LAPACK\index{LAPACK} libraries.

\item Use the Intel C++ compiler\index{Intel\,compiler} if
  available. Anecdotally, this tends to produce code that runs faster than
  \bitt{gcc}\index{gcc}. The \bitt{configure} script should automatically
  detect the Intel C++ compiler\index{compiler}, and use it if available. To
  use the Intel Math Kernel Library as well, which is not automatically
  detected, use the \bitt{--enable-mkl} command-line option.

\end{itemize}

\section{Style guide\label{Style_guide}}\index{style\,guide}

The following conventions are used for LibBi model files:
\begin{itemize}
\item Model names are CamelCase, the first letter always capitalised.
\item Action and block names are all lowercase, with multiple words separated
  by underscores.
\item Dimension and variable names should be consistent, where possible, with
  their counterparts in a description of the model as it might appear in a
  scientific paper. For example, single upper-case letters for the names of
  matrix\index{matrix} variables are appropriate, and standard symbols (rather
  than descriptive names) are encouraged. Greek letters should be written out
  in full, the first letter capitalised for the uppercase version
  (e.g. \bitt{gamma} and \bitt{Gamma}).
\item Comments should be used liberally, with descriptions provided for all
  dimensions and variables in particular. Consider including units as part of
  the description, where relevant.
\item Names ending in an underscore are intended for internal use only. They
  are not expected to be seen in a model file.
\item Indent using two spaces, and do not use tabs.\index{indenting}
\end{itemize}

Finally, use the \clientref{package} command to set up the standard files and
directory structure for a LibBi project. This will make your model and its
associated files easy to distribute, and your results easy to reproduce.
